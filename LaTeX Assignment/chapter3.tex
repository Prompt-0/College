% This is your third chapter.
\chapter{Probability and Information Theory} % Chapter 3 title from the reference book

\section*{Abstract}
%
% This abstract describes the "Probability and Information Theory" chapter (Chapter 3)
% of the "Deep Learning" book.
%
Probability theory is a fundamental tool for machine learning,
as it allows us to quantify and manage uncertainty.
This chapter introduces the core probabilistic concepts
needed to understand deep learning models.
We begin by defining random variables and probability
distributions, for both discrete and continuous cases.
Understanding these concepts is vital, as many models
are designed to output a probability distribution.
We also cover key ideas like marginal and conditional
probability, the chain rule of probability, and Bayes' rule,
which is the foundation for many generative models.
The chapter then introduces information theory, a branch
of mathematics built on probability.
We will define self-information, Shannon entropy,
and the Kullback-Leibler (KL) divergence.
These concepts are not just theoretical; they are
used to define the "loss functions" that train
neural networks.
For example, the cross-entropy function, which is
widely used in classification tasks, is derived
directly from these principles.
This chapter provides the language for reasoning
about uncertainty and information, which is
essential for all modern AI.